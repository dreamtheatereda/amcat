############################################################################          (C) Vrije Universiteit, Amsterdam (the Netherlands)            ##                                                                         ## This file is part of AmCAT - The Amsterdam Content Analysis Toolkit     ##                                                                         ## AmCAT is free software: you can redistribute it and/or modify it under  ## the terms of the GNU Affero General Public License as published by the  ## Free Software Foundation, either version 3 of the License, or (at your  ## option) any later version.                                              ##                                                                         ## AmCAT is distributed in the hope that it will be useful, but WITHOUT    ## ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or   ## FITNESS FOR A PARTICULAR PURPOSE. See the GNU Affero General Public     ## License for more details.                                               ##                                                                         ## You should have received a copy of the GNU Affero General Public        ## License along with AmCAT.  If not, see <http://www.gnu.org/licenses/>.  ############################################################################"""Script that adds Amcat articles to Solr, using multiple threadsBased on https://issues.apache.org/jira/browse/SOLR-1544"""import solrimport datetime, timefrom amcat.model import articlefrom django.db import connectionimport reimport threadingimport Queueimport logginglog = logging.getLogger(__name__)queue = Queue.Queue(10)class GMT1(datetime.tzinfo):    """very basic timezone object, needed for solrpy library.."""    def utcoffset(self,dt):        return datetime.timedelta(hours=1)    def tzname(self,dt):        return "GMT +1"    def dst(self,dt):        return datetime.timedelta(0)         class Worker(threading.Thread):    """A thread that handles adding data to Solr"""    def __init__(self, queue, threadNum):        self.queue = queue        self.threadNum = threadNum        self.solr = solr.SolrConnection('http://localhost:8983/solr')        threading.Thread.__init__(self)            self.hasProcessed = False    def run(self):        while True:            # thread blocks on get if nothing in the queue, sets taskdone immediately            # in case error with processing, don't block on join at end of script            log.debug("Thread " + self.threadNum + " waiting for queue")            task = self.queue.get()            self.queue.task_done()            # process a shutdown task by exiting the loop, hence the thread            if type(task) == str and task == "SHUTDOWN":                log.debug("Thread " + self.threadNum + " recieved shutdown task, exiting")                if self.hasProcessed: self.solr.commit() #only commit if thread has done something                break            self.hasProcessed = True            log.debug("Retrieved task from queue")            if executePosting(self.solr, task) == False:                break        log.debug('ending')      def stripChars(text):    """required to avoid:    SolrException: HTTP code=400, reason=Illegal character ((CTRL-CHAR, code 20))  at [row,col {unknown-source}]: [3519,150]    regexp copied from: http://mail-archives.apache.org/mod_mbox/lucene-solr-user/200901.mbox/%3C2c138bed0901040803x4cc07a29i3e022e7f375fc5f@mail.gmail.com%3E    """    if not text: return None    return  re.sub('[\x00-\x08\x0B\x0C\x0E-\x1F]', ' ', text);            def executePosting(solr, articles):    articleids = set([a.id for a in articles])    articlesToAdd = article.Article.objects.filter(pk__in=articleids, project__indexed=True, project__active=True)    articleidsToAdd = set([a.id for a in articlesToAdd])    articleidsToRemove = articleids - articleidsToAdd        log.debug("adding/updating %s articles, removing %s" % (len(articleidsToAdd), len(articleidsToRemove)))    if len(articleidsToAdd) > 0:        log.debug("finding sets") # TODO: make the code below nicer        cursor = connection.cursor()        cursor.execute("SELECT articleset_id, article_id FROM articlesets_articles WHERE article_id in (%s)" % ','.join(map(str, articleidsToAdd)))        rows = cursor.fetchall()        log.debug("%s sets found" % len(rows))        setsDict = {}        for row in rows:            articleid = row[1]            setid = row[0]            if not articleid in setsDict:                setsDict[articleid] = []            setsDict[articleid].append(setid)        log.debug("creating article dict")                    articlesDicts = []        for a in articlesToAdd:            articlesDicts.append(dict(id=a.id, headline=stripChars(a.headline),                                     body=stripChars(a.text), byline=stripChars(a.byline),                                     section=stripChars(a.section), projectid=a.project_id,                                    mediumid=a.medium_id, date=a.date.replace(tzinfo=GMT1()),                                    sets=setsDict.get(a.id))                                )                     log.debug("adding %s articles" % len(articlesDicts))        try:            solr.add_many(articlesDicts)        except Exception, e:            log.exception('Failed to add to Solr')            return False        if articleidsToRemove:            log.debug("deleting")        try:             solr.delete_many(articleidsToRemove)        except Exception, e:            log.exception('Failed to remove articles from Solr')            return False            return True    #solr.commit()        def index_articles(articlesQuerySet):            starttime = time.time()                # create a connection to a solr server    s = solr.SolrConnection('http://localhost:8983/solr')            count = articlesQuerySet.count()    log.debug("total number of articles: %s" % count)        if count == 0:        log.info('No articles found. Returning')        return    threadcount = 2    stepsize = 500    #stepsize = (count / threadcount) + 10    log.debug("stepsize: %s" % stepsize)    # start the worker threads which will block on the queue waiting for task    for i in range(threadcount):        Worker(queue, str(i)).start()    # put all of the data files into the queue, blocks add if the queue is full    # commits if the number of tasks processed is >= the commit size    for c in range(0, count, stepsize):        queue.put(articlesQuerySet[c:c+stepsize])    # wait for all the worker threads to complete current tasks    log.debug("Waiting for all tasks to complete")    queue.join()    # shutdown worker threads by putting shutdown task onto the queue    log.debug("Shutting down worker threads")    for i in range(threadcount):        queue.put("SHUTDOWN")    # wait for all the worker threads to complete current tasks    log.debug("Waiting for all tasks to complete")    queue.join()            # log.debug("optimizing")    # s.optimize()    endtime = time.time()    log.debug('Total time: %s' % (endtime-starttime))    log.debug("number of documents processed: %s" % count)    log.debug("docs per second: %s" % (count / max(int(endtime-starttime), 1)))